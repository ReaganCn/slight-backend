#!/usr/bin/env python3
"""
Comprehensive test script for URL Discovery and Social Media Integration.
Tests the complete workflow from URL discovery to social media data fetching.

üè∑Ô∏è PREDEFINED CATEGORIES SYSTEM:
The system now uses predefined categories that are controlled by admins/users:
- pricing: Pricing and subscription information
- features: Product features and capabilities  
- blog: Blog posts and articles
- about: Company information and team
- contact: Contact and support information
- social: Social media profiles
- careers: Career opportunities
- docs: Documentation and developer resources
- general: General website content

üìù CATEGORY SELECTION:
Edit the CATEGORIES_TO_SEARCH configuration below to select which categories to discover.
This allows you to focus on specific types of content for your competitive analysis.

This script demonstrates real-world URL discovery with examples like:
- Cursor.com (AI code editor): pricing, features, blog pages
- Notion.so: workspace features and pricing
- Linear.app: project management features

üéØ NEW BEHAVIOR: The system now returns only the SINGLE BEST URL per category:
1. Discovers multiple URLs per category (pricing, features, blog, etc.)
2. Groups URLs by category and gets top 3 candidates per category
3. Uses LLM (Cohere-first, OpenAI fallback) to select the best URL per category
4. Returns exactly one URL per category for clean, focused results

üìä SMART BATCHING SYSTEM (NEW):
- Processes URLs in batches of 10
- Calculates average confidence score per batch
- Continues to next batch only if avg confidence < 0.7
- Maximum 40 URLs total to control costs and processing time
- Early stopping when high-confidence results are found

The tests use Cohere-first AI strategy with OpenAI fallback for reliable operation.
"""

import asyncio
import sys
import os
import json
from typing import Dict, Any, List

# =============================================================================
# üè∑Ô∏è CATEGORY SELECTION CONFIGURATION
# =============================================================================

# üìù EDIT THIS LIST: Choose which categories to search for in all tests
# Available categories: pricing, features, blog, about, contact, social, careers, docs, general

# ‚úÖ DEFAULT CONFIGURATION (Business Intelligence Focus)
CATEGORIES_TO_SEARCH = ['pricing', 'features', 'blog', 'about']

# üîç COMPREHENSIVE SEARCH (Uncomment to search all categories)
# CATEGORIES_TO_SEARCH = ['pricing', 'features', 'blog', 'about', 'contact', 'social', 'careers', 'docs']

# üí∞ PRICING FOCUS (Uncomment for pricing-only analysis)
# CATEGORIES_TO_SEARCH = ['pricing']

# üéØ COMPETITIVE INTEL (Uncomment for competitive analysis focus)
# CATEGORIES_TO_SEARCH = ['pricing', 'features', 'about']

# üì± MARKETING FOCUS (Uncomment for content and social analysis)
# CATEGORIES_TO_SEARCH = ['blog', 'social', 'about']

# üë• RECRUITMENT FOCUS (Uncomment for hiring and company culture analysis)
# CATEGORIES_TO_SEARCH = ['careers', 'about', 'blog']

# =============================================================================
# TEST CONFIGURATION
# =============================================================================

# Real-world test companies (edit as needed)
TEST_COMPANIES = [
    # {
    #     'name': 'Linear',
    #     'website': 'https://linear.app',
    #     'description': 'Project management and issue tracking'
    # },
    # Add more test companies here as needed:
    # {
    #     'name': 'Cursor',
    #     'website': 'https://www.cursor.com',  
    #     'description': 'AI-powered code editor'
    # },
    {
        'name': 'Notion',
        'website': 'https://www.notion.so',
        'description': 'All-in-one workspace'
    }
]

# =============================================================================
# IMPORTS AND SETUP
# =============================================================================

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

from database import ensure_connection, get_session
from models import User, Competitor, CompetitorUrl, SocialMediaData
from services.url_discovery import URLDiscoveryService
from services.social_media import SocialMediaFetcher
from handlers.url_discovery import discover_urls, confirm_urls, get_discovered_urls
from handlers.social_media import fetch_social_data
from handlers.scrape_competitor import EnhancedCompetitorScraper
from handlers.competitor_management import create_competitor
from sqlalchemy import select, delete, text

def validate_categories():
    """Validate that selected categories are supported by the system"""
    print("üè∑Ô∏è Category Configuration Validation")
    print("=" * 50)
    
    # Get predefined categories from the service
    discovery_service = URLDiscoveryService()
    predefined_categories = discovery_service.get_predefined_categories()
    available_categories = list(predefined_categories.keys())
    
    print(f"üìã Available Categories ({len(available_categories)}):")
    for category, config in predefined_categories.items():
        desc = config['description']
        patterns = len(config['patterns'])
        print(f"   ‚Ä¢ {category}: {desc} ({patterns} patterns)")
    
    print(f"\nüéØ Selected Categories for Testing ({len(CATEGORIES_TO_SEARCH)}):")
    invalid_categories = []
    
    for category in CATEGORIES_TO_SEARCH:
        if category in available_categories:
            desc = predefined_categories[category]['description']
            print(f"   ‚úÖ {category}: {desc}")
        else:
            print(f"   ‚ùå {category}: INVALID - not in predefined categories")
            invalid_categories.append(category)
    
    if invalid_categories:
        print(f"\n‚ùå Invalid categories found: {invalid_categories}")
        print(f"   Please choose from: {available_categories}")
        return False
    
    print(f"\n‚úÖ All selected categories are valid!")
    return True

async def cleanup_test_data():
    """Clean up any existing test data"""
    print("üßπ Cleaning up test data...")
    
    async with get_session() as session:
        try:
            # Delete all foreign key references first to avoid constraint violations
            # Order matters: delete children before parents
            
            # 1. Delete scrape_results that reference competitor_urls
            await session.execute(
                text("""
                DELETE FROM scrape_results 
                WHERE competitor_url_id IN (
                    SELECT cu.id FROM competitor_urls cu
                    JOIN competitors c ON cu.competitor_id = c.id
                    WHERE c.name LIKE '%Test%'
                )
                """)
            )
            
            # 2. Delete scrape_jobs that reference competitors
            await session.execute(
                text("""
                DELETE FROM scrape_jobs 
                WHERE competitor_id IN (
                    SELECT id FROM competitors WHERE name LIKE '%Test%'
                )
                """)
            )
            
            # 3. Delete battle_cards that reference competitors
            await session.execute(
                text("""
                DELETE FROM battle_cards 
                WHERE user_id IN (
                    SELECT user_id FROM competitors WHERE name LIKE '%Test%'
                )
                """)
            )
            
            # 4. Delete social media data
            await session.execute(
                delete(SocialMediaData).where(
                    SocialMediaData.competitor_id.in_(
                        select(Competitor.id).where(Competitor.name.like('%Test%'))
                    )
                )
            )
            
            # 5. Delete competitor URLs
            await session.execute(
                delete(CompetitorUrl).where(
                    CompetitorUrl.competitor_id.in_(
                        select(Competitor.id).where(Competitor.name.like('%Test%'))
                    )
                )
            )
            
            # 6. Finally delete competitors
            await session.execute(
                delete(Competitor).where(Competitor.name.like('%Test%'))
            )
            
            await session.commit()
            print("‚úÖ Test data cleaned up")
        except Exception as e:
            await session.rollback()
            print(f"‚ö†Ô∏è Test data cleanup warning: {e}")
            # Don't fail the test for cleanup issues

async def test_database_connection():
    """Test database connection and schema"""
    print("\nüîç Testing database connection...")
    
    try:
        await ensure_connection()
        
        async with get_session() as session:
            # Test that new tables exist
            result = await session.execute(
                text("SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'competitor_urls')")
            )
            urls_table_exists = result.scalar()
            
            result = await session.execute(
                text("SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'social_media_data')")
            )
            social_table_exists = result.scalar()
            
            if urls_table_exists and social_table_exists:
                print("‚úÖ Database connection successful")
                print("‚úÖ New tables (competitor_urls, social_media_data) exist")
                return True
            else:
                print("‚ùå New tables missing - run migrations first")
                return False
                
    except Exception as e:
        print(f"‚ùå Database connection failed: {e}")
        return False

async def test_url_discovery_service():
    """Test the URL discovery service directly with predefined categories"""
    print("\nüîç Testing URL Discovery Service...")
    print(f"üè∑Ô∏è Searching for categories: {CATEGORIES_TO_SEARCH}")
    
    try:
        # Initialize service with Cohere-first strategy
        cohere_api_key = os.getenv('COHERE_API_KEY')
        openai_api_key = os.getenv('OPENAI_API_KEY')
        
        if not cohere_api_key and not openai_api_key:
            print("‚ö†Ô∏è No AI API keys - using mock discovery")
            return True
        
        discovery_service = URLDiscoveryService(
            cohere_api_key=cohere_api_key,
            openai_api_key=openai_api_key,
            google_cse_api_key=os.getenv('GOOGLE_CSE_API_KEY'),
            google_cse_id=os.getenv('GOOGLE_CSE_ID'),
            brave_api_key=os.getenv('BRAVE_API_KEY')
        )
        
        # Validate that our selected categories are supported
        predefined_categories = discovery_service.get_predefined_categories()
        available_categories = list(predefined_categories.keys())
        
        invalid_categories = [cat for cat in CATEGORIES_TO_SEARCH if cat not in available_categories]
        if invalid_categories:
            print(f"‚ùå Invalid categories selected: {invalid_categories}")
            print(f"   Available categories: {available_categories}")
            return False
        
        # Test URL discovery for a well-known company
        test_company = "Cursor"
        test_website = "https://www.cursor.com"
        
        print(f"\nüîç Discovering URLs for {test_company} ({test_website})...")
        print(f"   Searching for categories: {CATEGORIES_TO_SEARCH}")
        print("   üéØ Returns only the BEST URL per category (AI-selected)")
        print("   üìä Smart batching: 10 URLs/batch, avg confidence threshold 0.7 (max 40)")
        
        # This might take a while due to web searches
        discovered_urls = await discovery_service.discover_competitor_urls(
            test_company, test_website
        )
        
        print("‚úÖ URL Discovery Service working")
        print("üìä Discovery Results (1 URL per category):")
        
        # Display results by category
        categories_found = {}
        categories_discovered = set()
        
        for url_info in discovered_urls:
            category = url_info.get('category', 'uncategorized')
            url = url_info.get('url', '')
            confidence = url_info.get('confidence_score', 0)
            method = url_info.get('discovery_method', 'unknown')
            selection_method = url_info.get('selection_method', 'single_option')
            
            categories_discovered.add(category)
            categories_found[category] = url
            
            print(f"  üìÑ {category.upper()}: {url}")
            print(f"     Confidence: {confidence:.2f} | Discovery: {method} | Selection: {selection_method}")
        
        print(f"\nüìà Total categories found: {len(categories_found)}")
        
        # Check which of our selected categories were found
        print(f"\nüéØ Selected Categories Analysis:")
        found_selected = 0
        for category in CATEGORIES_TO_SEARCH:
            if category in categories_found:
                print(f"  ‚úÖ {category}: Found - {categories_found[category]}")
                found_selected += 1
            else:
                print(f"  ‚ö†Ô∏è {category}: Not found")
        
        success_rate = found_selected / len(CATEGORIES_TO_SEARCH) if CATEGORIES_TO_SEARCH else 1.0
        print(f"\nüìä Success Rate: {success_rate:.1%} ({found_selected}/{len(CATEGORIES_TO_SEARCH)} selected categories found)")
        
        # Validate all discovered categories are predefined
        invalid_discovered = categories_discovered - set(available_categories)
        if invalid_discovered:
            print(f"‚ùå Invalid categories discovered: {invalid_discovered}")
            print("   This indicates an issue with category validation in the service")
        else:
            print(f"‚úÖ All discovered categories are valid predefined categories")
        
        # Verify uniqueness (should be guaranteed now)
        unique_categories = len(set(url_info.get('category') for url_info in discovered_urls))
        total_urls = len(discovered_urls)
        print(f"\nüîç Uniqueness Check: {unique_categories} unique categories, {total_urls} total URLs")
        if unique_categories == total_urls:
            print("  ‚úÖ Perfect: One URL per category as expected")
        else:
            print("  ‚ö†Ô∏è Warning: Duplicate categories found")
        
        print(f"\nüìä Smart Batching Info:")
        print(f"  ‚Ä¢ Batch Size: 10 URLs per batch")
        print(f"  ‚Ä¢ Confidence Threshold: 0.7 (stop if avg >= 0.7)")
        print(f"  ‚Ä¢ Maximum URLs: 40 total")
        print(f"  ‚Ä¢ Early stopping helps reduce AI costs and processing time")
        
        return success_rate > 0.3  # Consider success if we find >30% of selected categories
        
    except Exception as e:
        print(f"‚ùå URL Discovery Service failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_real_world_examples():
    """Test URL discovery with real-world examples using selected categories"""
    print("\nüåç Testing Real-World URL Discovery Examples...")
    print("üéØ Each test returns the SINGLE BEST URL per category (AI-selected)")
    print("üìä Smart batching: 10 URLs/batch, continue only if avg confidence < 0.7 (max 40)")
    print(f"üè∑Ô∏è Searching for categories: {CATEGORIES_TO_SEARCH}")
    
    try:
        # Initialize service
        cohere_api_key = os.getenv('COHERE_API_KEY')
        openai_api_key = os.getenv('OPENAI_API_KEY')
        
        if not cohere_api_key and not openai_api_key:
            print("‚ö†Ô∏è No AI API keys - skipping real-world tests")
            return True
        
        discovery_service = URLDiscoveryService(
            cohere_api_key=cohere_api_key,
            openai_api_key=openai_api_key,
            google_cse_api_key=os.getenv('GOOGLE_CSE_API_KEY'),
            google_cse_id=os.getenv('GOOGLE_CSE_ID'),
            brave_api_key=os.getenv('BRAVE_API_KEY')
        )
        
        # Validate categories
        predefined_categories = discovery_service.get_predefined_categories()
        available_categories = list(predefined_categories.keys())
        
        invalid_categories = [cat for cat in CATEGORIES_TO_SEARCH if cat not in available_categories]
        if invalid_categories:
            print(f"‚ùå Invalid categories in CATEGORIES_TO_SEARCH: {invalid_categories}")
            return False
        
        results = {}
        
        for test_case in TEST_COMPANIES:
            print(f"\nüîç Testing: {test_case['name']} ({test_case['website']})")
            print(f"üìã Description: {test_case['description']}")
            print(f"üè∑Ô∏è Looking for categories: {CATEGORIES_TO_SEARCH}")
            print("üìä Using smart batching (10 URLs/batch, avg confidence threshold 0.7)")
            
            try:
                discovered_urls = await discovery_service.discover_competitor_urls(
                    test_case['name'], test_case['website']
                )
                
                # Analyze results
                categories_found = set(url_info.get('category', '') for url_info in discovered_urls)
                
                print(f"   üìä Found {len(discovered_urls)} URLs (1 per category)")
                print(f"   üìÇ Categories discovered: {list(categories_found)}")
                
                # Show each discovered URL with selection method
                for url_info in discovered_urls:
                    category = url_info.get('category', '')
                    url = url_info.get('url', '')
                    confidence = url_info.get('confidence_score', 0)
                    selection_method = url_info.get('selection_method', 'single_option')
                    print(f"   üìÑ {category}: {url}")
                    print(f"      Selection: {selection_method} | Confidence: {confidence:.2f}")
                
                # Check which of our selected categories were found
                found_selected = 0
                print(f"   üéØ Selected Categories Analysis:")
                for category in CATEGORIES_TO_SEARCH:
                    if category in categories_found:
                        found_selected += 1
                        print(f"      ‚úÖ {category}: Found")
                    else:
                        print(f"      ‚ö†Ô∏è {category}: Not found")
                
                success_rate = found_selected / len(CATEGORIES_TO_SEARCH) if CATEGORIES_TO_SEARCH else 1.0
                results[test_case['name']] = success_rate
                print(f"   üìà Success rate: {success_rate:.1%} ({found_selected}/{len(CATEGORIES_TO_SEARCH)} selected categories)")
                
                # Verify all categories are predefined
                invalid_discovered = categories_found - set(available_categories)
                if invalid_discovered:
                    print(f"   ‚ùå Invalid categories discovered: {invalid_discovered}")
                else:
                    print(f"   ‚úÖ All discovered categories are predefined")
                
                # Verify uniqueness
                if len(categories_found) == len(discovered_urls):
                    print(f"   ‚úÖ Uniqueness: Perfect (1 URL per category)")
                else:
                    print(f"   ‚ö†Ô∏è Uniqueness: Issue detected")
                
                # Show smart batching benefits
                print(f"   üìä Smart Batching Benefits:")
                print(f"      ‚Ä¢ Processed efficiently with early stopping")
                print(f"      ‚Ä¢ Reduced AI costs through intelligent batching")
                print(f"      ‚Ä¢ High-quality results with confidence thresholds")
                
            except Exception as e:
                print(f"   ‚ùå Failed to discover URLs: {e}")
                results[test_case['name']] = 0.0
        
        # Summary
        print(f"\nüìä Real-World Discovery Summary:")
        overall_success = sum(results.values()) / len(results) if results else 0
        for name, success_rate in results.items():
            print(f"  {name}: {success_rate:.1%}")
        print(f"  Overall Success Rate: {overall_success:.1%}")
        print(f"  Selected Categories: {CATEGORIES_TO_SEARCH}")
        
        return overall_success > 0.3  # Consider success if we find >30% of selected categories
        
    except Exception as e:
        print(f"‚ùå Real-world examples test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_full_workflow():
    """Test the complete URL discovery and social media workflow"""
    print("\nüîß Testing Full Workflow...")
    
    try:
        # Get or create test user
        async with get_session() as session:
            result = await session.execute(
                select(User).where(User.email == "test@example.com")
            )
            test_user = result.scalar_one_or_none()
            
            if not test_user:
                print("‚ùå Test user not found - run migrations first")
                return False
        
        # Step 1: Create test competitor
        print("üìù Step 1: Creating test competitor...")
        competitor_data = {
            'name': 'Test Cursor AI Editor',
            'website': 'https://www.cursor.com',
            'description': 'AI-powered code editor - test competitor for URL discovery'
        }
        
        competitor_result = await create_competitor(str(test_user.id), competitor_data)
        if not competitor_result['success']:
            print(f"‚ùå Failed to create competitor: {competitor_result}")
            return False
        
        competitor_id = competitor_result['competitor']['id']
        print(f"‚úÖ Created test competitor: {competitor_id}")
        
        # Step 2: Discover URLs
        print("üîç Step 2: Discovering URLs...")
        discovery_result = await discover_urls(competitor_id)
        
        if not discovery_result['success']:
            print(f"‚ùå URL discovery failed: {discovery_result}")
            return False
        
        print(f"‚úÖ URL discovery completed: {discovery_result['total_urls_found']} URLs found")
        
        # Step 3: Get discovered URLs
        print("üìã Step 3: Getting discovered URLs...")
        urls_result = await get_discovered_urls(competitor_id)
        
        if not urls_result['success']:
            print(f"‚ùå Failed to get URLs: {urls_result}")
            return False
        
        discovered_urls = urls_result['categorized_urls']
        print("üìä Discovered URLs by category:")
        for category, urls in discovered_urls.items():
            if urls:
                print(f"  {category}: {len(urls)} URLs")
        
        # Step 4: Confirm some URLs for testing
        print("‚úÖ Step 4: Confirming URLs...")
        confirmations = []
        
        # Confirm first URL from each category
        for category, urls in discovered_urls.items():
            if urls:
                confirmations.append({
                    'url_id': urls[0]['id'],
                    'status': 'confirmed'
                })
        
        if confirmations:
            confirmation_result = await confirm_urls(competitor_id, confirmations)
            if confirmation_result['success']:
                print(f"‚úÖ Confirmed {len(confirmations)} URLs")
            else:
                print(f"‚ùå URL confirmation failed: {confirmation_result}")
        
        # Step 5: Test enhanced scraping (if URLs confirmed)
        print("üï∑Ô∏è Step 5: Testing enhanced scraping...")
        if confirmations:
            try:
                async with EnhancedCompetitorScraper() as scraper:
                    scrape_result = await scraper.scrape_all_competitor_urls(competitor_id)
                    if scrape_result['success']:
                        print(f"‚úÖ Enhanced scraping completed: {scrape_result['summary']}")
                    else:
                        print(f"‚ö†Ô∏è Scraping completed with issues: {scrape_result}")
            except Exception as e:
                print(f"‚ö†Ô∏è Enhanced scraping failed (expected if no confirmed URLs): {e}")
        
        # Step 6: Test social media fetching (if social URLs found)
        print("üì± Step 6: Testing social media fetching...")
        social_urls = []
        for category, urls in discovered_urls.items():
            if category.startswith('social_') and urls:
                social_urls.extend(urls)
        
        if social_urls:
            try:
                social_result = await fetch_social_data(competitor_id)
                if social_result['success']:
                    print(f"‚úÖ Social media fetch completed: {social_result['summary']}")
                else:
                    print(f"‚ö†Ô∏è Social media fetch had issues: {social_result}")
            except Exception as e:
                print(f"‚ö†Ô∏è Social media fetch failed (expected without API keys): {e}")
        else:
            print("‚ö†Ô∏è No social media URLs found to test")
        
        print("\n‚úÖ Full workflow test completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Full workflow test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_social_media_service():
    """Test social media service with mock data"""
    print("\nüì± Testing Social Media Service...")
    
    try:
        # Test with minimal config (no real API keys)
        config = {
            'TWITTER_BEARER_TOKEN': os.getenv('TWITTER_BEARER_TOKEN'),
            'LINKEDIN_EMAIL': os.getenv('LINKEDIN_EMAIL'),
            'LINKEDIN_PASSWORD': os.getenv('LINKEDIN_PASSWORD'),
        }
        
        social_fetcher = SocialMediaFetcher(config=config)
        
        # Test with mock URLs
        mock_social_urls = [
            {
                'platform': 'social_twitter',
                'url': 'https://twitter.com/cursor',
                'url_id': 'mock-id-1'
            }
        ]
        
        # This will likely fail without real API keys, but tests the structure
        try:
            result = await social_fetcher.fetch_all_platforms('mock-competitor-id', mock_social_urls)
            print("‚úÖ Social Media Service structure working")
            print(f"üìä Result structure: {list(result.keys())}")
        except Exception as e:
            print(f"‚ö†Ô∏è Social Media Service failed (expected without API keys): {e}")
            print("‚úÖ Service structure is correct")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Social Media Service test failed: {e}")
        return False

async def run_all_tests():
    """Run all tests in sequence with category validation"""
    print("üöÄ Starting URL Discovery and Social Media Integration Tests")
    print("üè∑Ô∏è Using Predefined Categories System")
    print("=" * 60)
    
    # First validate category configuration
    if not validate_categories():
        print("‚ùå Category validation failed. Please fix CATEGORIES_TO_SEARCH configuration.")
        return False
    
    print("\n" + "=" * 60)
    
    tests = [
        ("Database Connection", test_database_connection),
        # ("URL Discovery Service", test_url_discovery_service),
        ("Real-World Examples", test_real_world_examples),
        # ("Social Media Service", test_social_media_service),
        # ("Full Workflow", test_full_workflow),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            result = await test_func()
            results[test_name] = result
        except Exception as e:
            print(f"‚ùå {test_name} test crashed: {e}")
            results[test_name] = False
    
    # Cleanup after tests
    await cleanup_test_data()
    
    # Print summary
    print("\n" + "=" * 60)
    print("üéØ TEST SUMMARY")
    print("=" * 60)
    
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {test_name}")
    
    total_passed = sum(results.values())
    total_tests = len(results)
    
    print(f"\nüìä Overall: {total_passed}/{total_tests} tests passed")
    print(f"üè∑Ô∏è Categories tested: {CATEGORIES_TO_SEARCH}")
    
    if total_passed == total_tests:
        print("üéâ All tests passed! URL Discovery with predefined categories is ready.")
    else:
        print("‚ö†Ô∏è Some tests failed. Check the output above for details.")
    
    return total_passed == total_tests

if __name__ == "__main__":
    print("üß™ URL Discovery and Social Media Integration Test Suite")
    print("üè∑Ô∏è Enhanced with Predefined Categories System")
    print("=" * 60)
    print("This tests the complete workflow from URL discovery to social media fetching.")
    print("üéØ Features real-world examples configurable via TEST_COMPANIES")
    print("ü§ñ Uses Cohere-first AI strategy with OpenAI fallback")
    print("üè∑Ô∏è Uses predefined categories controlled by admin/user configuration")
    print(f"üìù Currently searching for: {CATEGORIES_TO_SEARCH}")
    print("   Edit CATEGORIES_TO_SEARCH at the top of this file to customize")
    print()
    
    # Check required environment variables
    required_vars = ['DATABASE_URL']
    ai_vars = ['COHERE_API_KEY', 'OPENAI_API_KEY']
    
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    missing_ai_vars = [var for var in ai_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"‚ùå Missing required environment variables: {', '.join(missing_vars)}")
        print("These are required for the tests to run. Configure these in your .env file.")
        sys.exit(1)
    
    if len(missing_ai_vars) == len(ai_vars):
        print(f"‚ö†Ô∏è Missing AI API keys: {', '.join(ai_vars)}")
        print("URL discovery tests will use mock data. Configure at least one AI API key for full testing.")
        print()
    elif missing_ai_vars:
        print(f"‚ö†Ô∏è Missing optional AI API keys: {', '.join(missing_ai_vars)}")
        print("Tests will use available AI services. Configure both for full AI fallback testing.")
        print()
    
    try:
        success = asyncio.run(run_all_tests())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• Test suite crashed: {e}")
        sys.exit(1) 